<!DOCTYPE html>
<html class="html" lang="en-us" dir="ltr">
<head>
  
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>
  
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Llm Parameters | Leo&#39;s Page</title>


      <link rel="stylesheet" href="/css/style.min.90c0c98a5ecc51f823066c696d553c1d7a91daade21b9c030f94219b58c1c88d.css" integrity="sha256-kMDJil7MUfgjBmxpbVU8HXqR2q3iG5wDD5Qhm1jByI0=" crossorigin="anonymous">

<link rel="icon" href="/favicon.ico" />

<meta name="description" content="LLM Call Parameters: A Complete Guide
Large Language Models (LLMs) have become essential in powering applications like chatbots, content generators, code assistants, and more. When interacting with an LLM, understanding the various call parameters can help fine-tune the model’s behavior, improve the quality of responses, and control the output effectively. This guide dives deep into the most important parameters and how to use them effectively.
What Are LLM Call Parameters? # LLM call parameters are configuration settings that determine how the model generates output. These parameters allow you to control factors like creativity, relevance, and structure of the output.">
<meta property="og:url" content="https://regnull.com/posts/llm-parameters/">
  <meta property="og:site_name" content="Leo&#39;s Page">
  <meta property="og:title" content="Llm Parameters">
  <meta property="og:description" content="LLM Call Parameters: A Complete Guide
Large Language Models (LLMs) have become essential in powering applications like chatbots, content generators, code assistants, and more. When interacting with an LLM, understanding the various call parameters can help fine-tune the model’s behavior, improve the quality of responses, and control the output effectively. This guide dives deep into the most important parameters and how to use them effectively.
What Are LLM Call Parameters? # LLM call parameters are configuration settings that determine how the model generates output. These parameters allow you to control factors like creativity, relevance, and structure of the output.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-02-11T08:26:07-05:00">
    <meta property="article:modified_time" content="2025-02-11T08:26:07-05:00">


  <meta itemprop="name" content="Llm Parameters">
  <meta itemprop="description" content="LLM Call Parameters: A Complete Guide
Large Language Models (LLMs) have become essential in powering applications like chatbots, content generators, code assistants, and more. When interacting with an LLM, understanding the various call parameters can help fine-tune the model’s behavior, improve the quality of responses, and control the output effectively. This guide dives deep into the most important parameters and how to use them effectively.
What Are LLM Call Parameters? # LLM call parameters are configuration settings that determine how the model generates output. These parameters allow you to control factors like creativity, relevance, and structure of the output.">
  <meta itemprop="datePublished" content="2025-02-11T08:26:07-05:00">
  <meta itemprop="dateModified" content="2025-02-11T08:26:07-05:00">
  <meta itemprop="wordCount" content="1076">


</head>
<body class="body">
  <header class="header">
    <h1>Leo&#39;s Page</h1>
  <nav class="menu language">
    <ul class="menu__list language__list">
</ul>
  </nav>

  </header>
  <main class="main">
    
  <h1>Llm Parameters</h1>
  
  <time class="published-date" datetime="2025-02-11T08:26:07-05:00">2025-02-11</time>
  

  




  <details class="toc">
    <summary class="toc__summary">Table of Contents</summary>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#what-are-llm-call-parameters">What Are LLM Call Parameters?</a></li>
    <li><a href="#1-temperature">1. <strong>Temperature</strong></a>
      <ul>
        <li><a href="#mathematical-expression"><strong>Mathematical Expression</strong></a></li>
      </ul>
    </li>
    <li><a href="#2-top-p-nucleus-sampling">2. <strong>Top-p (Nucleus Sampling)</strong></a></li>
    <li><a href="#3-min-p-minimum-probability">3. <strong>Min-p (Minimum Probability)</strong></a></li>
    <li><a href="#4-max-tokens">4. <strong>Max Tokens</strong></a></li>
    <li><a href="#5-frequency-penalty">5. <strong>Frequency Penalty</strong></a></li>
    <li><a href="#6-presence-penalty">6. <strong>Presence Penalty</strong></a></li>
    <li><a href="#7-top-k-sampling-less-frequently-used">7. <strong>Top-k Sampling</strong> (Less Frequently Used)</a></li>
    <li><a href="#8-top-a-sampling-less-frequently-used">8. <strong>Top-a Sampling</strong> (Less Frequently Used)</a></li>
    <li><a href="#9-typical-p-typical-decoding-less-frequently-used">9. <strong>Typical-p (Typical Decoding)</strong> (Less Frequently Used)</a></li>
    <li><a href="#10-repetition-penalty">10. <strong>Repetition Penalty</strong></a></li>
    <li><a href="#11-stop-sequences">11. <strong>Stop Sequences</strong></a></li>
    <li><a href="#tuning-parameters-for-optimal-output"><strong>Tuning Parameters for Optimal Output</strong></a>
      <ul>
        <li><a href="#example-configuration-for-creative-writing">Example Configuration for Creative Writing:</a></li>
        <li><a href="#example-configuration-for-summarization">Example Configuration for Summarization:</a></li>
      </ul>
    </li>
    <li><a href="#sources">Sources</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
  </details>

  <p>LLM Call Parameters: A Complete Guide</p>
<p>Large Language Models (LLMs) have become essential in powering applications like chatbots, content generators, code assistants, and more. When interacting with an LLM, understanding the various call parameters can help fine-tune the model’s behavior, improve the quality of responses, and control the output effectively. This guide dives deep into the most important parameters and how to use them effectively.</p>
<hr>
<h2 class="heading" id="what-are-llm-call-parameters">
  What Are LLM Call Parameters?<span class="heading__anchor"> <a href="#what-are-llm-call-parameters">#</a></span>
</h2><p>LLM call parameters are configuration settings that determine how the model generates output. These parameters allow you to control factors like creativity, relevance, and structure of the output.</p>
<p>Different LLM providers (like OpenAI, Hugging Face, etc.) offer various parameters, but common ones include:</p>
<ul>
<li><strong>Temperature</strong></li>
<li><strong>Top-p (nucleus sampling)</strong></li>
<li><strong>Min-p (minimum probability)</strong></li>
<li><strong>Max tokens</strong></li>
<li><strong>Frequency penalty</strong></li>
<li><strong>Presence penalty</strong></li>
<li><strong>Top-k sampling</strong></li>
<li><strong>Top-a (additional sampling adjustment)</strong></li>
<li><strong>Typical-p (typical decoding)</strong></li>
<li><strong>Repetition penalty</strong></li>
<li><strong>Stop sequences</strong></li>
</ul>
<p>Let’s break down each of these in detail.</p>
<hr>
<h2 class="heading" id="1-temperature">
  1. <strong>Temperature</strong><span class="heading__anchor"> <a href="#1-temperature">#</a></span>
</h2><p>The <code>temperature</code> parameter controls the randomness of the model’s output. It affects how the model selects words when generating a response.</p>
<ul>
<li><strong>Range:</strong> 0 to 1 (sometimes higher)</li>
<li><strong>Low temperature (e.g., 0.0 - 0.3):</strong> Produces deterministic, precise outputs with less variation. Useful for tasks requiring accuracy, like coding.</li>
<li><strong>High temperature (e.g., 0.7 - 1.0):</strong> Encourages creativity and diversity in responses. Ideal for creative writing, brainstorming, and story generation.</li>
</ul>
<h3 class="heading" id="mathematical-expression">
  <strong>Mathematical Expression</strong><span class="heading__anchor"> <a href="#mathematical-expression">#</a></span>
</h3><p>When using temperature, logits (raw prediction scores) are transformed into probabilities using the softmax function:</p>
\[ P(x_i) = \frac{\exp(\frac{logit_i}{T})}{\sum_j \exp(\frac{logit_j}{T})} \]<p>Where:</p>
<ul>
<li>\( logit_i \) is the raw score for token \( i \)</li>
<li>\( T \) is the temperature value</li>
<li>The denominator ensures that the probabilities sum to 1.</li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;temperature&#34;</span>: <span style="color:#ae81ff">0.7</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 class="heading" id="2-top-p-nucleus-sampling">
  2. <strong>Top-p (Nucleus Sampling)</strong><span class="heading__anchor"> <a href="#2-top-p-nucleus-sampling">#</a></span>
</h2><p><code>top_p</code> (also known as <strong>nucleus sampling</strong>) defines the probability threshold for selecting words. Instead of considering the entire vocabulary, the model only selects from the top words that together have a cumulative probability close to <code>p</code>.</p>
<ul>
<li><strong>Range:</strong> 0 to 1</li>
<li><strong>Low top-p (e.g., 0.1 - 0.3):</strong> Restricts output to highly probable words, resulting in more predictable responses.</li>
<li><strong>High top-p (e.g., 0.8 - 1.0):</strong> Allows a broader selection of words, enhancing creativity and diversity.</li>
</ul>
<p><strong>Tip:</strong> When using <code>top_p</code>, consider reducing <code>temperature</code> to balance control and creativity.</p>
<p><strong>Example:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;top_p&#34;</span>: <span style="color:#ae81ff">0.9</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 class="heading" id="3-min-p-minimum-probability">
  3. <strong>Min-p (Minimum Probability)</strong><span class="heading__anchor"> <a href="#3-min-p-minimum-probability">#</a></span>
</h2><p><code>min_p</code> sets a lower bound on the probability of tokens considered during generation. This ensures that only words with a probability higher than the specified threshold are selected, filtering out extremely low-probability options.</p>
<ul>
<li><strong>Range:</strong> 0 to 1</li>
<li><strong>Higher min-p (e.g., 0.1 - 0.3):</strong> Eliminates low-probability words, leading to more focused and confident outputs.</li>
<li><strong>Lower min-p (e.g., 0.0 - 0.05):</strong> Allows for more diversity and creativity but may introduce noise or less relevant responses.</li>
</ul>
<p><strong>Tip:</strong> <code>min_p</code> can be useful when combined with <code>top_p</code> to balance diversity and coherence.</p>
<p><strong>Example:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;min_p&#34;</span>: <span style="color:#ae81ff">0.05</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 class="heading" id="4-max-tokens">
  4. <strong>Max Tokens</strong><span class="heading__anchor"> <a href="#4-max-tokens">#</a></span>
</h2><p><code>max_tokens</code> controls the maximum length of the generated output. The token count includes both the input and output tokens.</p>
<ul>
<li><strong>1 token:</strong> Approximately 4 characters (depending on the language).</li>
<li><strong>Limit:</strong> Different models have varying token limits (e.g., OpenAI’s GPT-4 supports up to 8,192 tokens in some configurations).</li>
</ul>
<p><strong>Tip:</strong> Be mindful of the token limit to avoid truncated responses.</p>
<p><strong>Example:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;max_tokens&#34;</span>: <span style="color:#ae81ff">500</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 class="heading" id="5-frequency-penalty">
  5. <strong>Frequency Penalty</strong><span class="heading__anchor"> <a href="#5-frequency-penalty">#</a></span>
</h2><p><code>frequency_penalty</code> discourages the model from repeating words or phrases by penalizing high-frequency terms.</p>
<ul>
<li><strong>Range:</strong> -2.0 to 2.0</li>
<li><strong>Positive values:</strong> Encourage more variety by reducing repetition.</li>
<li><strong>Negative values:</strong> Encourage repetition of key phrases or words.</li>
</ul>
<p><strong>Use Case:</strong> Creative writing, summarization, or any task where repetition is undesirable.</p>
<p><strong>Example:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;frequency_penalty&#34;</span>: <span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 class="heading" id="6-presence-penalty">
  6. <strong>Presence Penalty</strong><span class="heading__anchor"> <a href="#6-presence-penalty">#</a></span>
</h2><p><code>presence_penalty</code> affects how likely the model is to introduce new topics or words that haven’t appeared in the input.</p>
<ul>
<li><strong>Range:</strong> -2.0 to 2.0</li>
<li><strong>Positive values:</strong> Encourage the introduction of new content.</li>
<li><strong>Negative values:</strong> Restrict the model to existing topics, which is useful for focused conversations or technical explanations.</li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;presence_penalty&#34;</span>: <span style="color:#ae81ff">0.3</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 class="heading" id="7-top-k-sampling-less-frequently-used">
  7. <strong>Top-k Sampling</strong> (Less Frequently Used)<span class="heading__anchor"> <a href="#7-top-k-sampling-less-frequently-used">#</a></span>
</h2><p><code>top_k</code> limits the model to selecting from the top <code>k</code> most likely tokens at each generation step.</p>
<ul>
<li><strong>Range:</strong> 1 to the size of the model’s vocabulary</li>
<li><strong>Low top-k values (e.g., 10-50):</strong> Promote conservative, focused responses.</li>
<li><strong>High top-k values:</strong> Allow more diverse outputs but may introduce irrelevant content.</li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;top_k&#34;</span>: <span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 class="heading" id="8-top-a-sampling-less-frequently-used">
  8. <strong>Top-a Sampling</strong> (Less Frequently Used)<span class="heading__anchor"> <a href="#8-top-a-sampling-less-frequently-used">#</a></span>
</h2><p><code>top_a</code> adjusts the probability mass dynamically by focusing on tokens with adaptive probability constraints.</p>
<ul>
<li><strong>Range:</strong> 0 to 1 (provider-specific implementation)</li>
<li><strong>Higher values:</strong> Promote more exploratory or creative outputs.</li>
<li><strong>Lower values:</strong> Keep outputs more controlled and predictable.</li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;top_a&#34;</span>: <span style="color:#ae81ff">0.8</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 class="heading" id="9-typical-p-typical-decoding-less-frequently-used">
  9. <strong>Typical-p (Typical Decoding)</strong> (Less Frequently Used)<span class="heading__anchor"> <a href="#9-typical-p-typical-decoding-less-frequently-used">#</a></span>
</h2><p><code>typical_p</code> ensures that the model selects tokens based on how typical they are within the overall distribution of predicted words.</p>
<ul>
<li><strong>Range:</strong> 0 to 1</li>
<li><strong>Low values:</strong> Result in safer and more conservative responses.</li>
<li><strong>Higher values:</strong> Allow for more diverse generation without randomness.</li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;typical_p&#34;</span>: <span style="color:#ae81ff">0.9</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 class="heading" id="10-repetition-penalty">
  10. <strong>Repetition Penalty</strong><span class="heading__anchor"> <a href="#10-repetition-penalty">#</a></span>
</h2><p><code>repetition_penalty</code> discourages the model from repeating the same phrases or words excessively.</p>
<ul>
<li><strong>Range:</strong> &gt;1.0 (values like 1.1 to 2.0 are common)</li>
<li><strong>Higher values:</strong> Stronger penalty on repeated tokens.</li>
</ul>
<p><strong>Use Case:</strong> Useful for longer conversations or tasks where variety is critical.</p>
<p><strong>Example:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;repetition_penalty&#34;</span>: <span style="color:#ae81ff">1.2</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 class="heading" id="11-stop-sequences">
  11. <strong>Stop Sequences</strong><span class="heading__anchor"> <a href="#11-stop-sequences">#</a></span>
</h2><p><code>stop</code> defines sequences of characters or words that signal the model to stop generating text.</p>
<ul>
<li><strong>Use Case:</strong> To prevent the model from generating text beyond a certain point or after a specific phrase.</li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;stop&#34;</span>: [<span style="color:#e6db74">&#34;\n&#34;</span>, <span style="color:#e6db74">&#34;End&#34;</span>]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 class="heading" id="tuning-parameters-for-optimal-output">
  <strong>Tuning Parameters for Optimal Output</strong><span class="heading__anchor"> <a href="#tuning-parameters-for-optimal-output">#</a></span>
</h2><p>Here are a few general tips for tuning LLM call parameters:</p>
<ul>
<li><strong>Structured outputs:</strong> Use low <code>temperature</code> and specific <code>stop</code> sequences.</li>
<li><strong>Creative outputs:</strong> Increase <code>temperature</code>, <code>top_p</code>, or <code>typical_p</code> for variety.</li>
<li><strong>Avoiding repetitive content:</strong> Adjust <code>frequency_penalty</code>, <code>presence_penalty</code>, and <code>repetition_penalty</code> accordingly.</li>
<li><strong>Token limits:</strong> Ensure <code>max_tokens</code> is set appropriately for the task.</li>
</ul>
<h3 class="heading" id="example-configuration-for-creative-writing">
  Example Configuration for Creative Writing:<span class="heading__anchor"> <a href="#example-configuration-for-creative-writing">#</a></span>
</h3><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;temperature&#34;</span>: <span style="color:#ae81ff">0.8</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;top_p&#34;</span>: <span style="color:#ae81ff">0.9</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;min_p&#34;</span>: <span style="color:#ae81ff">0.05</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;max_tokens&#34;</span>: <span style="color:#ae81ff">1000</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;frequency_penalty&#34;</span>: <span style="color:#ae81ff">0.2</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;presence_penalty&#34;</span>: <span style="color:#ae81ff">0.5</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;top_k&#34;</span>: <span style="color:#ae81ff">50</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;typical_p&#34;</span>: <span style="color:#ae81ff">0.9</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;repetition_penalty&#34;</span>: <span style="color:#ae81ff">1.2</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;stop&#34;</span>: [<span style="color:#e6db74">&#34;END&#34;</span>]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h3 class="heading" id="example-configuration-for-summarization">
  Example Configuration for Summarization:<span class="heading__anchor"> <a href="#example-configuration-for-summarization">#</a></span>
</h3><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;temperature&#34;</span>: <span style="color:#ae81ff">0.3</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;top_p&#34;</span>: <span style="color:#ae81ff">0.8</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;min_p&#34;</span>: <span style="color:#ae81ff">0.0</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;max_tokens&#34;</span>: <span style="color:#ae81ff">300</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;frequency_penalty&#34;</span>: <span style="color:#ae81ff">0.5</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;presence_penalty&#34;</span>: <span style="color:#ae81ff">0.0</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;top_k&#34;</span>: <span style="color:#ae81ff">10</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;repetition_penalty&#34;</span>: <span style="color:#ae81ff">1.1</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 class="heading" id="sources">
  Sources<span class="heading__anchor"> <a href="#sources">#</a></span>
</h2><ul>
<li><a href="https://openrouter.ai/docs/api-reference/parameters">Open Router parameters description</a></li>
</ul>
<hr>
<h2 class="heading" id="conclusion">
  Conclusion<span class="heading__anchor"> <a href="#conclusion">#</a></span>
</h2><p>Understanding and optimizing LLM call parameters is key to getting the desired output from large language models. Whether you are generating code, writing creative stories, or providing technical summaries, fine-tuning parameters like <code>temperature</code>, <code>top_p</code>, <code>min_p</code>, <code>typical_p</code>, and <code>repetition_penalty</code> will help you get the best results. Experimentation is crucial, so don’t be afraid to try different configurations to see what works best for your application.</p>

  

  

  
  <nav class="page-nav">
      <a class="page-nav__next-link" href="/posts/my-first-post/">Next: My First Post</a>
  </nav>

  

  </main>
  <footer class="footer">
    
<p class="footer__copyright-notice">&copy; 2025 </p>
<p class="footer__theme-info">Built with <a href='https://gohugo.io'>Hugo</a> and <a href='https://github.com/CyrusYip/hugo-theme-yue'>Yue</a></p>
  </footer>
  
</body>
</html>
